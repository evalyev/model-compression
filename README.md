# model-compression
Домашки по курсу ИТМО Методы компрессии нейросетевых моделей

**PyTorch:**.
- Время вывода: 5,84 секунды
- Размер модели: 246,9 Кб
- Accuracy: 0.5844
- Precision: 0.5844
- Recall: 0.5844
- F1-Score: 0.5825

**ONNX:**
- Время вывода: 7,70 секунды
- Размер модели: 243,9 Кб
- Accuracy: 0.5844
- Precision: 0.5844
- Recall: 0.5844
- F1-Score: 0.5825

**OpenVINO:**
- Время вывода: 9,28 секунды
- Размер модели: 121,1 Кб (LeNet.bin), 19,0 Кб (LeNet.xml)
- Accuracy: 0.5843
- Precision: 0.5843
- Recall: 0.5843
- F1-Score: 0.5824


1. **Время вывода:**
   - PyTorch имеет самое быстрое время вывода, а OpenVINO - самое медленное. Такое различие может быть объяснено методами оптимизации, используемыми каждым фреймворком.
   - ONNX находится между PyTorch и OpenVINO по времени вывода, что говорит о том, что модель ONNX сохраняет некоторые характеристики производительности модели.

2. **Размер модели:**.
   - Наименьший размер модели имеет OpenVINO с отдельными bin- и xml-файлами. Модель ONNX имеет несколько больший размер, чем OpenVINO. PyTorch имеет самый большой размер модели. Меньший размер модели OpenVINO является преимуществом при хранении и развертывании.

3. **Метрики точности:**.
   - Показатели Accuracy, Precision, Recall и F1-Score одинаковы для всех трех методов. Это говорит о том, что производительность модели остается одинаковой независимо от используемого фреймворка.

**Плюсы использования ONNX:**
- ONNX является функционально совместимым форматом, позволяющим легко переключаться между различными фреймворками глубокого обучения.
- Он обеспечивает переносимость моделей, позволяя переносить их с одной платформы на другую с минимальными изменениями.
- Производительность близка к PyTorch, что говорит о том, что модель ONNX сохраняет большую часть эффективности исходной модели.

**Плюсы использования OpenVINO:**
- OpenVINO предоставляет инструменты для оптимизации моделей, делая их эффективными для вычислений на оборудовании Intel.
- Она поддерживает широкий спектр процессоров Intel, включая CPU, GPU и VPU.
- Размер модели меньше, что может быть полезно для развертывания в средах с ограниченными ресурсами.

PyTorch является самым быстрым, но может иметь больший размер модели. ONNX обеспечивает совместимость моделей. OpenVINO оптимизирует модели для эффективного вывода на оборудовании Intel, при этом размер модели меньше. Выбор метода зависит от ваших конкретных требований и целевой платформы развертывания. Если вам необходимо развертывание на оборудовании Intel, то предпочтительным вариантом может быть OpenVINO. Если же приоритетом является переносимость моделей, то отличным выбором может стать ONNX.
